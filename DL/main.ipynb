{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DevOps\n",
    "\n",
    "I will be using the 128x128 CNN model for my DevOps project. To make it easier to quickly glance the important information I have made this section. \n",
    "\n",
    "### Retraining Model\n",
    "\n",
    "Since we are done finding the best model and will now be using it for a production use case, I think it is better to train the model on the whole dataset rather than a smaller portion. \n",
    "\n",
    "#### Setup\n",
    "\n",
    "I will import the libraries and also get the preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Visualisation\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# Miscellaneous\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Modelling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# Others\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"./Vegetable Images\"\n",
    "CLASSES = ['Broccoli','Capsicum','Bottle_Gourd','Radish','Tomato','Brinjal','Pumpkin','Carrot','Papaya','Cabbage','Bitter_Gourd','Cauliflower','Bean','Cucumber','Potato']\n",
    "tfClasses = tf.constant(CLASSES)\n",
    "TRAIN = tf.data.Dataset.list_files(f\"{ROOT}/train/*/*\")\n",
    "TEST = tf.data.Dataset.list_files(f\"{ROOT}/test/*/*\")\n",
    "VAL = tf.data.Dataset.list_files(f\"{ROOT}/validation/*/*\")\n",
    "dataCats = [\"train\",\"test\",\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classCounts = []\n",
    "for cat in dataCats:\n",
    "    class_distro = [len(tf.data.Dataset.list_files(f\"{ROOT}/{cat}/{i}/*\")) for i in CLASSES]\n",
    "    classCounts.append(class_distro)\n",
    "    \n",
    "train_distro = classCounts[0]\n",
    "\n",
    "targetSize = max(train_distro)\n",
    "additional_needed = [targetSize-i for i in train_distro]\n",
    "\n",
    "aug_train = []\n",
    "for i,v in enumerate(CLASSES):\n",
    "    path = f'{ROOT}/train/{v}'\n",
    "    imgNeeded = additional_needed[i]\n",
    "    images = os.listdir(path)[:imgNeeded]\n",
    "    aug_train.extend([path + \"/\" + i for i in images])\n",
    "train_data_aug = tf.data.Dataset.from_tensor_slices(aug_train)\n",
    "\n",
    "def createPreprocessor(imgSize):\n",
    "    def processing(path):\n",
    "        label = tf.strings.split(path , os.path.sep)\n",
    "        one_hot = label[-2] == tfClasses\n",
    "        label = tf.argmax(one_hot)\n",
    "        label = tf.one_hot(label, depth=len(CLASSES))\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [imgSize, imgSize])\n",
    "        img = tf.image.rgb_to_grayscale(img)\n",
    "        img = img / 255.0\n",
    "        return img , label\n",
    "    return processing\n",
    "    \n",
    "def augmentation(image,label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.rot90(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    return image , label\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "  ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size=1000)\n",
    "  ds = ds.batch(32)\n",
    "  ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "train_ori_large = TRAIN.map(createPreprocessor(128))\n",
    "train_aug_large = TRAIN.map(createPreprocessor(128)).map(augmentation)\n",
    "train_large_ds = train_ori_large.concatenate(train_aug_large)\n",
    "\n",
    "test_large_ds = TEST.map(createPreprocessor(128))\n",
    "val_large_ds = VAL.map(createPreprocessor(128))\n",
    "\n",
    "data = train_large_ds.concatenate(test_large_ds).concatenate(val_large_ds)\n",
    "data = configure_for_performance(data)\n",
    "\n",
    "train_ori_small = TRAIN.map(createPreprocessor(31))\n",
    "train_aug_small = TRAIN.map(createPreprocessor(31)).map(augmentation)\n",
    "train_small_ds = train_ori_small.concatenate(train_aug_small)\n",
    "\n",
    "test_small_ds = TEST.map(createPreprocessor(31))\n",
    "val_small_ds = VAL.map(createPreprocessor(31))\n",
    "\n",
    "data_small = train_small_ds.concatenate(test_small_ds).concatenate(val_small_ds)\n",
    "data_small = configure_for_performance(data_small)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
