{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DevOps\n",
    "\n",
    "> If you are a teacher and feel bored please read Docs/DeepLearning.md where I documented the nightmares I went through during this project. Also look at the beautiful README I created, it looks nicer on GitLab\n",
    "\n",
    "I will be using the 128x128 CNN model for my DevOps project. To make it easier to quickly glance the important information I have made this section. \n",
    "\n",
    "## Development Environment Setup\n",
    "\n",
    "- Ensure that you have Docker installed, if not download it at this [website](https://docs.docker.com/engine/install/)\n",
    "- Once you are done, run the following commands\n",
    "\n",
    "``` bash \n",
    "docker pull python:3.9.18\n",
    "docker run -it -v /var/run/docker.sock:/var/run/docker.sock --name CNN_Server python:3.9.18 sh -c \"apt-get update ; apt-get install docker.io -y ; bash \n",
    "```\n",
    "\n",
    "> Note: If you are using Mac or Linux, these commands will work instantly, but if you are using Windows, you might have to enable Windows Subsystem for Linux before proceeding\n",
    "\n",
    "To make sure your docker works inside the development container, try running `docker info`\n",
    "\n",
    "- Check your Docker Desktop to see if the CNN_Server container is running\n",
    "- Open up Visual Studio Code and install \"Remote Explorer\" extension if you don't have it\n",
    "\n",
    "<img src=\"./Docs/Screenshots/RemoteExplorer.png\" width=\"400\">\n",
    "\n",
    "- Navigate to the Explorer tab and click on the icon as shown below\n",
    "\n",
    "<img src=\"./Docs/Screenshots/DevContainer.png\" width=\"500\">\n",
    "\n",
    "## Configuring Development Environment\n",
    "\n",
    "-  Run the sequence of commands below to download the project directory inside your development container and configure python with all the required packages.\n",
    "\n",
    "``` bash\n",
    "git clone https://gitlab.com/4618-devops/ca2-daaa2b01-2214618-jeyakumarsriram-dl.git\n",
    "python -m venv env\n",
    "source ./env/bin/activate\n",
    "cd /ca2-daaa2b01-2214618-jeyakumarsriram-dl\n",
    "python -m pip install tensorflow\n",
    "pip install matplotlib seaborn numpy pytest\n",
    "```\n",
    "\n",
    "- To train the models, you would need the dataset too. it is recommended to use the dataset from [this website](https://www.kaggle.com/datasets/moustacheman/vegetable-images)\n",
    "\n",
    "Or alternatively if you have the Kaggle API acces you can run \n",
    "\n",
    "```bash\n",
    "kaggle datasets download -d moustacheman/vegetable-images \n",
    "```\n",
    "\n",
    "- After which, you should see a zip file, unzip it and place it into your project directory. Ensure it is named \"Vegetable Images\" if not change the `ROOT` variable below. \n",
    "\n",
    "After all these steps, you should be ready to train some models!\n",
    "\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Run the following cells sequentially. Ensure that the kernl you selected is Venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# Miscellaneous\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Modelling\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `ROOT` if your data directory is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"./Vegetable Images\"\n",
    "CLASSES = ['Broccoli','Capsicum','Bottle_Gourd','Radish','Tomato','Brinjal','Pumpkin','Carrot','Papaya','Cabbage','Bitter_Gourd','Cauliflower','Bean','Cucumber','Potato']\n",
    "tfClasses = tf.constant(CLASSES)\n",
    "TRAIN = tf.data.Dataset.list_files(f\"{ROOT}/train/*/*\")\n",
    "TEST = tf.data.Dataset.list_files(f\"{ROOT}/test/*/*\")\n",
    "VAL = tf.data.Dataset.list_files(f\"{ROOT}/validation/*/*\")\n",
    "dataCats = [\"train\",\"test\",\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing some preprocessing on the images. If you are using a different dataset, edit this part as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classCounts = []\n",
    "for cat in dataCats:\n",
    "    class_distro = [len(tf.data.Dataset.list_files(f\"{ROOT}/{cat}/{i}/*\")) for i in CLASSES]\n",
    "    classCounts.append(class_distro)\n",
    "    \n",
    "train_distro = classCounts[0]\n",
    "\n",
    "targetSize = max(train_distro)\n",
    "additional_needed = [targetSize-i for i in train_distro]\n",
    "\n",
    "aug_train = []\n",
    "for i,v in enumerate(CLASSES):\n",
    "    path = f'{ROOT}/train/{v}'\n",
    "    imgNeeded = additional_needed[i]\n",
    "    images = os.listdir(path)[:imgNeeded]\n",
    "    aug_train.extend([path + \"/\" + i for i in images])\n",
    "train_data_aug = tf.data.Dataset.from_tensor_slices(aug_train)\n",
    "\n",
    "def createPreprocessor(imgSize):\n",
    "    def processing(path):\n",
    "        label = tf.strings.split(path , os.path.sep)\n",
    "        one_hot = label[-2] == tfClasses\n",
    "        label = tf.argmax(one_hot)\n",
    "        label = tf.one_hot(label, depth=len(CLASSES))\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [imgSize, imgSize])\n",
    "        img = tf.image.rgb_to_grayscale(img)\n",
    "        img = img / 255.0\n",
    "        return img , label\n",
    "    return processing\n",
    "    \n",
    "def augmentation(image,label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.rot90(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    return image , label\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "  ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size=1000)\n",
    "  ds = ds.batch(32)\n",
    "  ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "train_ori_large = TRAIN.map(createPreprocessor(128))\n",
    "train_aug_large = TRAIN.map(createPreprocessor(128)).map(augmentation)\n",
    "train_large_ds = train_ori_large.concatenate(train_aug_large)\n",
    "\n",
    "test_large_ds = TEST.map(createPreprocessor(128))\n",
    "val_large_ds = VAL.map(createPreprocessor(128))\n",
    "\n",
    "data = train_large_ds.concatenate(test_large_ds).concatenate(val_large_ds)\n",
    "data = configure_for_performance(data)\n",
    "\n",
    "train_ori_small = TRAIN.map(createPreprocessor(31))\n",
    "train_aug_small = TRAIN.map(createPreprocessor(31)).map(augmentation)\n",
    "train_small_ds = train_ori_small.concatenate(train_aug_small)\n",
    "\n",
    "test_small_ds = TEST.map(createPreprocessor(31))\n",
    "val_small_ds = VAL.map(createPreprocessor(31))\n",
    "\n",
    "data_small = train_small_ds.concatenate(test_small_ds).concatenate(val_small_ds)\n",
    "data_small = configure_for_performance(data_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be defining our 2 models here. One accepts a 128x128 size image input while the other takes a 31x31 input. Both will only use grayscale images as specified in the previous assignment brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cnn_model = tf.keras.Sequential([\n",
    "    Conv2D(16, 3, activation='relu', input_shape=(31, 31, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    Conv2D(32, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5), \n",
    "    Dense(15, activation='softmax')\n",
    "])\n",
    "\n",
    "reg_cnn_model_large = tf.keras.Sequential([\n",
    "    Conv2D(32, 3, activation='relu', input_shape=(128, 128, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    Conv2D(128, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    Conv2D(256, 3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5), \n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5), \n",
    "    \n",
    "    Dense(15, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a summary of the inputs of the various layers of the CNN models. The number of parameters in each is also mentioned. Note that models with higher parameters will take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cnn_model.summary()\n",
    "reg_cnn_model_large.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be saving the models using the SavedModel format rather than h5 or .keras in order to ensure compatibility for Tensorflow Serving. The model training begins now, estimated wait time is 20 minutes. So have a nap or a small meal while it cooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-6)\n",
    "\n",
    "reg_cnn_model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "modelHist = reg_cnn_model.fit(\n",
    "      data_small, epochs=25, callbacks=[es,reduce_lr]\n",
    "    )\n",
    "\n",
    "reg_cnn_model.save(\"./cnn_small/1\", save_format=\"tf\")\n",
    "\n",
    "reg_cnn_model_large.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "modelHist2 = reg_cnn_model_large.fit(\n",
    "      data, epochs=15, callbacks=[es,reduce_lr]\n",
    "    )\n",
    "\n",
    "reg_cnn_model_large.save(\"./cnn_large/1\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, I have written some code to see the difference in speed and accuracy between the two models, As you can see the larger model is slightly slower but more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure prediction time for final_31\n",
    "start_time = time.time()\n",
    "prediction_31 = reg_cnn_model.evaluate(configure_for_performance(test_small_ds))\n",
    "end_time = time.time()\n",
    "time_taken_31 = end_time - start_time\n",
    "\n",
    "# Measure prediction time for final_128\n",
    "start_time = time.time()\n",
    "prediction_128 = reg_cnn_model_large.evaluate(configure_for_performance(test_large_ds))\n",
    "end_time = time.time()\n",
    "time_taken_128 = end_time - start_time\n",
    "\n",
    "# Print the prediction times\n",
    "print(f\"Prediction time for final_31: {time_taken_31} seconds\")\n",
    "print(f\"Prediction time for final_128: {time_taken_128} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving\n",
    "\n",
    "Ensure that you have `cnn_large/` and `cnn_small/` directories in your project's root folder. \n",
    "\n",
    "- Run this command to pull the tensorflow serving docker image\n",
    "\n",
    "``` bash\n",
    "docker pull tensorflow/serving\n",
    "```\n",
    "\n",
    "If you are using a M Series Macbook, use this command instead to ensure compatibility\n",
    "\n",
    "```bash\n",
    "docker pull emacski/tensorflow-serving:latest-linux_arm64\n",
    "```\n",
    "\n",
    "These are two tensorflow models that we will be serving using the same container. Since we are not serving just one model, we cannot use the standard command for tensorflow/serving. \n",
    "\n",
    "- Create a `model_config.config` file in the root of your project directory. \n",
    "\n",
    "> For convenience, consider copying your `model_config.config`, `cnn_large/` and `cnn_small/` files to a easily accessible place like `C://production` in windows and `~/production` in Unix based systems. \n",
    "\n",
    "- Add the following YAML code to your config\n",
    "\n",
    "```yaml\n",
    "model_config_list:\n",
    "  config:\n",
    "    name: \"large\"\n",
    "    base_path: \"/models/cnn_large\"\n",
    "    model_platform: \"tensorflow\"\n",
    "  config:\n",
    "    name: \"small\"\n",
    "    base_path: \"/models/cnn_small\"\n",
    "    model_platform: \"tensorflow\" \n",
    "```\n",
    "\n",
    "- Run the following command to serve your models. Replace the string with your system's information before running the command\n",
    "\n",
    "```bash\n",
    "docker run --name cnn_models -p 8501:8501 \\\n",
    "    -v \"/Users/sriramjeyakumar/Production/model_config.config:/models/model_config.config\" \\\n",
    "    -v \"/Users/sriramjeyakumar/Production/cnn_large:/models/cnn_large\" \\\n",
    "    -v \"/Users/sriramjeyakumar/Production/cnn_small:/models/cnn_small\" \\\n",
    "    -t emacski/tensorflow-serving:latest-linux_arm64 --model_config_file=/models/model_config.config\n",
    "```\n",
    "\n",
    "> You should change `emacski/tensorflow-serving:latest-linux_arm64` to `tensorflow/serving` if you are not using a M Series Mac\n",
    "\n",
    "Here is the breakdown of the command above:\n",
    "    - We map the port 8501 to our localhost's port 8501 so we can make requests to it\n",
    "    - We mount 3 different files: Our two model directories and the config file\n",
    "    - We use the serving image to create the container\n",
    "    - We specify to TF Serving the config file\n",
    "\n",
    "You should expect to see something like this when it runs\n",
    "\n",
    "<img src=\"./Docs/Screenshots/Serve%20Two%20Models.png\" alt=\"serving\" width=\"500\">\n",
    "\n",
    "Also run thw following commands to connect your development container to the Tensorflow Server so you have call its api from within your development environment. \n",
    "\n",
    "```bash\n",
    "apt-get install iputils-ping\n",
    "docker network create dl_network\n",
    "docker network connect dl_network cnn_models\n",
    "docker network connect dl_network CNN_Server\n",
    "```\n",
    "\n",
    "Try `ping cnn_models` and you should see something like this \n",
    "\n",
    "<img src=\"./Docs/Screenshots/DL_Network_Ping.png\" alt=\"ping\" width=\"500\">\n",
    "\n",
    "\n",
    "## Checking if TensorFlow Serving is Running\n",
    "\n",
    "1. **Large Model:**\n",
    "   - URL: [http://localhost:8501/v1/models/large](http://localhost:8501/v1/models/large)\n",
    "   - This endpoint should provide information about the 128x128 TensorFlow model, including its status and configuration.\n",
    "\n",
    "2. **Small Model:**\n",
    "   - URL: [http://localhost:8501/v1/models/small](http://localhost:8501/v1/models/small)\n",
    "   - This endpoint should provide information about the 31x31 TensorFlow model, including its status and configuration.\n",
    "\n",
    "Open these URLs in your web browser or use tools like `curl` or `wget` in the command line to make HTTP requests. If TensorFlow Serving is running and the models are successfully loaded, you should see something like this\n",
    "\n",
    "<img src=\"https://i.ibb.co/LN19B9T/cnn-large-api.png\" alt=\"CNN Large API\" width=\"500\">\n",
    "<img src=\"https://i.ibb.co/bP21QLF/cnn-small-api.png\" alt=\"CNN Small API\" width=\"500\">\n",
    "\n",
    "\n",
    "Also run this command that will use pytest to check various parts of our model. It will test the basic functionalities, test the range of the model as well as its consistency.\n",
    "\n",
    "```bash\n",
    "python -m pytest\n",
    "```\n",
    "\n",
    "You should expect to see something like this.\n",
    "\n",
    "<img src=\"./Docs/Screenshots/PyTestInit.png\" alt=\"pytest\" width=\"500\">\n",
    "\n",
    "\n",
    "Congrats! ðŸ¥³ You have successfully served your models.\n",
    "\n",
    "Now they are only accessible in your laptop. If you want to make it accessible everywhere, consider using a cloud solution like [Render](render.com) to host your model\n",
    "\n",
    "\n",
    "## Deployment\n",
    "\n",
    "Before you do any deployment, you need your changes to be pushed to the cloud. But before you commit your changes so far into git, we have to address how you are storing your models. \n",
    "\n",
    "Tensorflow models are not small and are more like static object which are not supposed to be trakced by git. So it is not the best idea to push your entire model onto github using the conventional way. \n",
    "\n",
    "While there a probably better industry-standard ways to store your models, most of them are PAID ðŸ˜­. Since you my friend, have no cash to spare, I will give you a simpler solution that solves this problem. Git Large File Storage. \n",
    "\n",
    "Git LFS basically allows git to take in your files but not trakc all the changes. When you push your repo to the platform like GitHub or GitLab, items marked with LFS wont be stored in the same place as the rest of your code. Instead it will be stored in a LFS server and a pointer to it will be stored in your main repo. Dont worry, when you push and pull, Git will automatically get your LFS items for you. \n",
    "\n",
    "### LFS Setup\n",
    "\n",
    "For Windows, skip this section as it is installed by default for you, if it isn't for some reason, good luck!\n",
    "\n",
    "For Macbook, you can run `brew install git-lfs`\n",
    "\n",
    "But if you are working on this inside a Development container,  you are in a linux environment. You will have to run more disgusting commands\n",
    "\n",
    "```bash\n",
    "curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n",
    "apt-get install git-lfs\n",
    "```\n",
    "\n",
    "> We omitted the use of sudo here as its not needed for docker containers\n",
    "\n",
    "Confirm installation using the command `git lfs`\n",
    "\n",
    "Now, which files are so huge that we need Git LFS to track them? We you might think that it is the entire directories of `cnn_large/` and `cnn_small/` but NO. If you snoop around into these directories, most of the files basically contain metadata, like you model structure and whatnot. The real heavy files are those under `variables` subdirectory. So we will only use LFS on them. Now you should run the following commands.\n",
    "\n",
    "```bash\n",
    "git lfs install\n",
    "git lfs track \"*/*/variables/*\"\n",
    "git add .gitattributes\n",
    "```\n",
    "\n",
    "And that's it! You have setup LFS, now you just have to add and commit as usual. Whenever you create new models, remember to run the track command again. \n",
    "\n",
    "If yyou are unsure if you have done this correctly, run\n",
    "\n",
    "```bash\n",
    "git lfs status\n",
    "```\n",
    "which will show the files that are tracked.\n",
    "\n",
    "One more thing, under you GitLab ensure that the LFS option is selected like this \n",
    "\n",
    "<img src=\"./Docs/Screenshots/Git%20LFS%20Setting.png\" alt=\"render\" width=\"500\">\n",
    "\n",
    "When you deployed to gitlab you will see a Tag like this to signifiy that it is under the LFS server\n",
    "\n",
    "<img src=\"./Docs/Screenshots/GitLFS_gitlab.png\" alt=\"success\" width=\"500\">\n",
    "\n",
    "### Deploying to Render\n",
    "\n",
    "Ensure that you have committed all your changes to GitLab. You either create a new branch called `releases` like me or you can just use the main branch for deployment, whatever works for you. I'm treating my `main` branch more like a development branch while `releases` is the final branch for me. But again, whatever works for you.\n",
    "\n",
    "I would also suggest that you make  `releases` a protected branch so it can't just be deleted liek this \n",
    "\n",
    "<img src=\"./Docs/Screenshots/Protected%20Branch.png\" alt=\"success\" width=\"500\">\n",
    "\n",
    "To deploy, \n",
    "\n",
    "- Create an account on render and log in. \n",
    "- After going to your dashboard, click \"New\" and select \"Web Service\" as the type of project you will be deploying\n",
    "- Connect your GitLab to Render for easier deployment\n",
    "- Fill in the name and make sure the Environment is Docker, set the render to trakc `releases` branch or whatever you want\n",
    "- Click deploy and wait for the logs to say your service is live\n",
    "\n",
    "You should see something like this if you model deployment was successful.\n",
    "\n",
    "<img src=\"./Docs/Screenshots/Deployed.png\" alt=\"success\" width=\"500\">\n",
    "\n",
    "Now, edit the pytest/conftest.py file and replace the cnn_model:8501 with the url of your render application and rerun PyTest. You should expect to see something like this.\n",
    "\n",
    "<img src=\"./Docs/Screenshots/RenderPytest.png\" alt=\"render\" width=\"500\">\n",
    "\n",
    "\n",
    "### Setting Up CI/CD Pipeline\n",
    "\n",
    "This is to basically make you like easier. Whenever you commit into the Releases branch, you want to make sure you code works before it deploys to the internet. Therefore I create a pipeline to test the code before deploying to the production server. \n",
    "\n",
    "There are many ways to test a Tensorflow Serving app. I unsuccessfully tried running docker inside the CICD pipeline and test. Therefore, once again, I will show you a less fancy but crud way of solving this problem. Having two deployments: one for testing, one for production.\n",
    "\n",
    "Let me explain the process to you. When a commit happens in the `releases` branch, the following will happen.\n",
    "\n",
    "1. GitLab will trigger the redeployment of your code in a Test Server hosted on Render (separate from the one you made)\n",
    "2. GitLab will run PyTest on that test server to confirm if it works as intended\n",
    "3. If the PyTest returns a success code of 0, Gitlab will trigger the redeployment of your main production server\n",
    "\n",
    "The configuration for the Test and Production server are exactly the same so you can test your code without messing up the main server. For this you would need to do a couple of things.\n",
    "\n",
    "- Setup another render deployment called Test_Server. Follow the exactl same steps you followed just now but different name\n",
    "- Go to the Settings of both Render deployments, scroll down and disable automatic redeploy. This is to make Render wait for you call before deploying something.\n",
    "<img src=\"./Docs/Screenshots/Deploy%20Hook.png\" alt=\"render\" width=\"500\">\n",
    "\n",
    "- Copy the Deploy Hook URL into a safe place\n",
    "- In your gitlab project, go to Settings/CICD and expand the Variables tab. Under which you should add two variables using the two deploy hooks as shown below.\n",
    "<img src=\"./Docs/Screenshots/Secrets%20Gitlab.png\" alt=\"render\" width=\"500\">\n",
    "- Now edit your `tests/conftest` file and change the url to your Test Server's url\n",
    "- Create a `.gitlab-ci.yml` file in your project's root folder and put the following things in there.\n",
    "\n",
    "```yaml\n",
    "stages:\n",
    "  - test_deploy\n",
    "  - test\n",
    "  - deploy\n",
    "\n",
    "test_deployment:\n",
    "  stage: test_deploy\n",
    "  script:\n",
    "    - curl -s \"$TEST_DEPLOY\"\n",
    "    - sleep 1m\n",
    "  only:\n",
    "    - releases\n",
    "\n",
    "pytest:\n",
    "  stage: test\n",
    "  image: python:3.9.18\n",
    "  script:\n",
    "    - pip install -r requirements.txt\n",
    "    - python -m pytest --junitxml=report.xml\n",
    "    - TEST_CODE=$?\n",
    "    - |\n",
    "      if [ $TEST_CODE -eq 0 ]; then\n",
    "        echo \"Tests are all passed!\"\n",
    "      else\n",
    "        echo \"Tests failed\"\n",
    "        sleep 1m\n",
    "        exit 1\n",
    "      fi\n",
    "  artifacts:\n",
    "    when: always\n",
    "    reports:\n",
    "      junit: report.xml\n",
    "  retry:\n",
    "    max: 2\n",
    "    when: \n",
    "      - always\n",
    "  only:\n",
    "    - releases\n",
    "  dependencies:\n",
    "    - test_deployment\n",
    "\n",
    "deployment:\n",
    "  stage: deploy\n",
    "  script:\n",
    "    - curl -s \"$PROD_DEPLOY\"\n",
    "  only:\n",
    "    - releases\n",
    "  dependencies:\n",
    "    - pytest\n",
    "```\n",
    "\n",
    "This pipeline basically does what I said. To point out a few things, the testing stage will retry twice before quitting and with a 1 minite delay inbetween. This delay is included bcause something Render takes very long to deploy and a delay in render deployment shouldn't cause the pipeline to fail. I will also save the PyTest in a report.xml file so that you can view it later. To trigger the redeployment, all you have to do is send a GET request to the Deploy Hook you copied. This can be done using curl. To assess the success of PyTest, we will be utilising the status code. 0 means all tests have passed. Only in that scenario we will deploy the application, if not we will just stop the pipeline using `exit 1`. \n",
    "\n",
    "Now you can try to make a commit in the `releases` branch. To see your pipeline running go to Build/Pipelines in your GitLab interface. You should seem something like this\n",
    "\n",
    "<img src=\"./Docs/Screenshots/Pipeline%20Success.png\" alt=\"pipeline success\" width=\"500\">\n",
    "\n",
    "\n",
    "And if you click on the test stage, you can view the results like this \n",
    "\n",
    "<img src=\"./Docs/Screenshots/CICD%20Pytest.png\" alt=\"CICD pytest\" width=\"500\">\n",
    "\n",
    "If you move over to render you will see the logs mentioned that it was redeployed like this\n",
    "\n",
    "<img src=\"./Docs/Screenshots/Production%20Render%20Server%20Redeploy.png\" alt=\"CICD pytest\" width=\"500\">\n",
    "\n",
    "\n",
    "That's it! You are done!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
